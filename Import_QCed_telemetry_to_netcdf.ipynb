{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Import modules\n",
    "# ipython magic to plot in line\n",
    "%matplotlib inline\n",
    "#import mpld3\n",
    "#mpld3.enable_notebook()\n",
    "import matplotlib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import xarray as xr\n",
    "from astropy.io import ascii\n",
    "import pytz\n",
    "# OS interaction\n",
    "import sys\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Directories\n",
    "# Path to raw data\n",
    "main_dir   = os.path.normpath(r'F:\\Work\\e\\Data\\Obs\\Canada_Project_Sites\\Nov_2014_snow_storm_data')\n",
    "# Ascii input folder\n",
    "dir_in     = main_dir + '\\QC_ascii'\n",
    "# netcdf output folder\n",
    "dir_out    = main_dir + '\\\\QC_netcdf'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Define input format of ascii files\n",
    "input_format = 'CRHO_TELM'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "if input_format == 'CRHO_TELM':\n",
    "    # Ascii data format info\n",
    "    c_header = 4 # Header lines\n",
    "    c_column_line = 1 # line where column names start\n",
    "    c_delimiter = ','\n",
    "    # time zone variables\n",
    "    #tz_in = pytz.timezone('Canada/Mountain')\n",
    "    tz_in = pytz.timezone('Etc/GMT-6')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Get file in info\n",
    "os.chdir(dir_in) # Move to input\n",
    "content = os.listdir(os.getcwd()) # Get list of files\n",
    "num_files = len([name for name in os.listdir('.') if os.path.isfile(name)]) # Get number of files in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#dat = ascii.read(cfile,header_start=c_column_line,data_start=c_header,delimiter=c_delimiter,exclude_names='N/A',guess=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing BNS\n",
      "need to get correct site location info, make these variables\n",
      "Frozen(SortedKeysDict({'site': 1, 'time': 50980}))\n",
      "Processing CRN\n",
      "need to get correct site location info, make these variables\n",
      "Frozen(SortedKeysDict({'site': 1, 'time': 50980}))\n",
      "Processing FRS\n",
      "need to get correct site location info, make these variables\n",
      "Frozen(SortedKeysDict({'site': 1, 'time': 50980}))\n",
      "Processing PWL\n",
      "need to get correct site location info, make these variables\n",
      "Frozen(SortedKeysDict({'site': 1, 'time': 50980}))\n"
     ]
    }
   ],
   "source": [
    "# Read in each file\n",
    "ds_ALL = []\n",
    "for cfile in content:\n",
    "    # Get current station name\n",
    "    csta_name = cfile[0:3] # Take the first three letter abbreviation as the name\n",
    "    print('Processing ' + csta_name)\n",
    "    \n",
    "    # Import data to pandas dataframe\n",
    "    dat = ascii.read(cfile,header_start=c_column_line,data_start=c_header,delimiter=c_delimiter,exclude_names='N/A')\n",
    "    datain = pd.DataFrame(dat.as_array())\n",
    "    \n",
    "    # Make TIMESTAMP the index\n",
    "    datain['TIMESTAMP'] = datain['TIMESTAMP'].astype('datetime64[ns]')\n",
    "    datain = datain.set_index('TIMESTAMP')\n",
    "    \n",
    "    # Set time zone\n",
    "    datain.index = datain.index.tz_localize(tz_in)\n",
    "    \n",
    "    # Import header info\n",
    "    headerinfo = pd.read_csv(cfile,nrows=2,skiprows=1)\n",
    "    \n",
    "    # Convert to xray\n",
    "    ds = xr.Dataset.from_dataframe(datain)\n",
    "    \n",
    "    # Rename time\n",
    "    ds.rename({'TIMESTAMP':'time'},inplace=True)\n",
    "    \n",
    "    # Add new site dimension and make record\n",
    "    ds = xr.concat([ds],'site',data_vars='all')\n",
    "    #ds_site['site'] = csta_name\n",
    "\n",
    "    # Add Coords\n",
    "    print('need to get correct site location info, make these variables')\n",
    "    #c_LAT xr.DataArray(39.0,['site',0])\n",
    "    #ds = xr.concat({'LAT':('site',[39.0])})\n",
    "    ds.coords['LAT'] = 39.0\n",
    "    ds.coords['LON'] = -120.0\n",
    "    #ds.coords['sta_name'] = csta_name\n",
    "    \n",
    "    # Add variable attributes (units and sample period)\n",
    "    for cvar in ds.data_vars:\n",
    "        ds.get(cvar).attrs['unit']   = headerinfo.loc[0,[cvar]].values[0]\n",
    "        #ds.get(cvar).attrs['sample'] = headerinfo.loc[1,[cvar]].values[0]\n",
    "        \n",
    "    print(ds.dims)\n",
    "    ds_ALL.append(ds)\n",
    "    \n",
    "    # Export to netcdf\n",
    "    cfileout = os.path.join(dir_out,csta_name + '.nc')\n",
    "    ds.to_netcdf(cfileout,format='netcdf4') #,encoding={'AirTemp_Avg': {'dtype': 'int16', 'scale_factor': 0.1, '_FillValue': -9999}})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#ds_combined = xr.concat(ds_ALL,dim='site')\n",
    "#ds_combined = xr.merge(ds_ALL,dim='site')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#ds_merged = ds_ALL[0]\n",
    "#ds_merged.merge(ds_ALL[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Open and combine multiple files\n",
    "#os.chdir(dir_out)\n",
    "#ds_ALL = xr.open_mfdataset('*.nc',chunks=10,concat_dim='site')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import collections\n",
    "d = collections.OrderedDict()\n",
    "d['site1'] = ds_ALL[0]\n",
    "d['site2'] = ds_ALL[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\new356\\Anaconda3\\lib\\site-packages\\ipykernel\\__main__.py:1: FutureWarning: the `dim` argument to `concat` will be required in a future version of xarray; for now, setting it to the old default of 'concat_dim'\n",
      "  if __name__ == '__main__':\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "can only concatenate xarray Dataset and DataArray objects",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-20-91e69fd0848f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mxr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconcat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0md\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mC:\\Users\\new356\\Anaconda3\\lib\\site-packages\\xarray\\core\\combine.py\u001b[0m in \u001b[0;36mconcat\u001b[1;34m(objs, dim, data_vars, coords, compat, positions, indexers, mode, concat_over)\u001b[0m\n\u001b[0;32m    110\u001b[0m         \u001b[0mf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_dataset_concat\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    111\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 112\u001b[1;33m         raise TypeError('can only concatenate xarray Dataset and DataArray '\n\u001b[0m\u001b[0;32m    113\u001b[0m                         'objects')\n\u001b[0;32m    114\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobjs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata_vars\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcoords\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcompat\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpositions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: can only concatenate xarray Dataset and DataArray objects"
     ]
    }
   ],
   "source": [
    "X = xr.concat(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#ds2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
