{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Import modules\n",
    "# ipython magic to plot in line\n",
    "%matplotlib inline\n",
    "#import mpld3\n",
    "#mpld3.enable_notebook()\n",
    "import matplotlib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime\n",
    "import xarray as xr\n",
    "from astropy.io import ascii\n",
    "import pytz\n",
    "# OS interaction\n",
    "import sys\n",
    "import os\n",
    "import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# http://stackoverflow.com/questions/38987/how-can-i-merge-two-python-dictionaries-in-a-single-expression\n",
    "def merge_two_dicts(x, y):\n",
    "    '''Given two dicts, merge them into a new dict as a shallow copy.'''\n",
    "    z = x.copy()\n",
    "    z.update(y)\n",
    "    return z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Directories\n",
    "# Path to raw data\n",
    "out_dir    = os.path.normpath(r'F:\\Work\\e\\Data\\Obs\\Canada_Project_Sites\\Nov_2014_snow_storm_data')\n",
    "main_dir   = os.path.normpath(r'C:\\Users\\new356\\Google Drive\\Nov2014 Data QC Completed Data')\n",
    "# MetaDataFile\n",
    "meta_data_file  = os.path.join(main_dir,'CRHO_Station_lat_long_elevation.txt')\n",
    "# Ascii input folder\n",
    "dir_in     = main_dir + '\\QC_Data_ASCII_slim'\n",
    "# netcdf output folder\n",
    "dir_out    = out_dir + '\\\\QC_netcdf'\n",
    "# all data out\n",
    "cfileout         = os.path.join(dir_out,'CRHO.nc')\n",
    "# Just fortress Nov. 2014 storm out\n",
    "cfileout_storm   = os.path.join(dir_out,'CRHO_Nov_2014_Storm.nc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Define input format of ascii files\n",
    "input_format = 'CRHO_TELM'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "if input_format == 'CRHO_TELM':\n",
    "    # Ascii data format info\n",
    "    c_header = 4 # Header lines\n",
    "    c_column_line = 1 # line where column names start\n",
    "    c_delimiter = ','\n",
    "    # time zone variables\n",
    "    #tz_in = pytz.timezone('Etc/GMT-6')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Get file in info\n",
    "os.chdir(dir_in) # Move to input\n",
    "content = glob.glob('*.txt') # Get list of files\n",
    "num_files = len(content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Read in metadata (all stations)\n",
    "metadata = pd.read_csv(meta_data_file,index_col='station')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['BNS_15min_2013_2016_slim.txt',\n",
       " 'BRP_15min_2014_2016_slim.txt',\n",
       " 'BWH_15min_2014_slim.txt',\n",
       " 'CRG_15min_2014_slim.txt',\n",
       " 'CRN_15min_2013_2016_slim.txt',\n",
       " 'FLG_15min_2014_2016_slim.txt',\n",
       " 'FRG_15min_2014_slim.txt',\n",
       " 'FRS_15min_2013_2016_slim.txt',\n",
       " 'HLN_15min_2014_2015_slim.txt',\n",
       " 'PWL_15min_2013_2016_slim.txt',\n",
       " 'PYT_15min_2013_2016_slim.txt',\n",
       " 'SIB_15min_2014_slim.txt']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Initalize stuff\n",
    "c_dict = {}\n",
    "stations_all=[]\n",
    "variables=[]\n",
    "units_all = {}\n",
    "time_index = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing BNS\n",
      "Processing BRP\n",
      "Processing BWH\n",
      "Processing CRG\n",
      "Processing CRN\n",
      "Processing FLG\n",
      "Processing FRG\n",
      "Processing FRS\n",
      "Processing HLN\n",
      "Processing PWL\n",
      "Processing PYT\n",
      "Processing SIB\n"
     ]
    }
   ],
   "source": [
    "# Read in each file\n",
    "for cfile in content:\n",
    "    \n",
    "    # Get current station name\n",
    "    csta_name = cfile[0:3] # Take the first three letter abbreviation as the name\n",
    "    print('Processing ' + csta_name)\n",
    "    stations_all.append(csta_name)\n",
    "    \n",
    "    # Import data to pandas dataframe\n",
    "    dat = ascii.read(cfile,header_start=c_column_line,data_start=c_header,delimiter=c_delimiter,exclude_names='N/A')\n",
    "    datain = pd.DataFrame(dat.as_array())\n",
    "    \n",
    "    # Alternate method that drops duplicate (second variables) (Not correct but works for now)\n",
    "    #print(\"Using temp fix to process dataframes with duplicate columns. DATA NOT CORRECT!!!!\")\n",
    "    #datain = pd.read_csv(content[4],header=c_column_line) #,mangle_dupe_cols=False)\n",
    "    #datain.drop(datain.index[:2], inplace=True)\n",
    "    #datain = datain[datain.columns.drop(datain.filter(regex='.1'))]\n",
    "    #datain.columns\n",
    "    \n",
    "    \n",
    "    # Replace -9999 with nan (recomended by netcdf)\n",
    "    datain.replace(-9999,np.NaN,inplace=True)\n",
    "    \n",
    "    # Make TIMESTAMP the index\n",
    "    datain['TIMESTAMP'] = datain['TIMESTAMP'].astype('datetime64[ns]')\n",
    "    datain = datain.set_index('TIMESTAMP')\n",
    "    \n",
    "    # Set time zone\n",
    "    #datain.index = datain.index.tz_localize(tz_in)\n",
    "    \n",
    "    # Import header info \n",
    "    headerinfo = pd.read_csv(cfile,nrows=2,skiprows=1)\n",
    "    units = headerinfo.loc[0,:].tolist() # Grab first row of dataframe (units)\n",
    "    units = units[1:] # Remove first value which is the units of the timestamp\n",
    "    units_dic = dict(zip(datain.columns,units)) # Dictionary of variable:units for this stations\n",
    "    units_all = merge_two_dicts(units_all, units_dic) # Merge dictoinaries together (units_dic overwrites any units_all)\n",
    "    \n",
    "    # Loop through all variables for this station\n",
    "    c_variables = datain.columns\n",
    "    variables.extend(c_variables.values) # Store all variables for use latter\n",
    "    for c_var in c_variables:\n",
    "        c_dict[(csta_name,c_var)]        =pd.DataFrame(datain[c_var])\n",
    "        c_dict[(csta_name,c_var)].columns=[c_var]\n",
    "        c_dict[(csta_name,c_var)].index  = datain.index\n",
    "        \n",
    "    # Save time index for each station (need to fill in missing variables later)\n",
    "    time_index[csta_name] = datain.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Get unique variables from list variables\n",
    "variables_uniq = set(variables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Soil Moisture C\n",
      "Downward Solar Radiation\n",
      "Soil Moisture E\n",
      "Total Pressure Adjusted to Sea-level\n",
      "Air Moisture Content A\n",
      "Wind Direction at A\n",
      "Scalar Wind Speed B\n",
      "Scalar Wind Speed A\n",
      "Soil Moisture A\n",
      "Upward Terrestrial Rad\n",
      "Soil Temperature E\n",
      "Snow Depth A\n",
      "Snow Water Equivelent A\n",
      "Soil Temperature B\n",
      "Soil Moisture D\n",
      "Downward Terrestrial Rad\n",
      "Incremental Precipitation B\n",
      "Soil Temperature A\n",
      "Snow Layer Temperature A\n",
      "Soil Temperature D\n",
      "Soil Moisture B\n",
      "Soil Temperature C\n",
      "Total Pressure Unadjusted A\n",
      "Air temperature A\n",
      "Soil Heat Flux  A\n",
      "Incremental Precipitation A\n",
      "Snow Depth QC value\n",
      "Upward Solar Radiation\n"
     ]
    }
   ],
   "source": [
    "# Extract data for each variable from the dictionary and create a xray.Dataset\n",
    "\n",
    "ds_list = [] # Initalize list of xray Datasets (each a different variable)\n",
    "\n",
    "# For each unique variable in the dictionary\n",
    "for c_var in variables_uniq:\n",
    "    print(c_var)\n",
    "    all_vars={} # Initialize dictionary that only contains one variable for all stations\n",
    "    # For each station\n",
    "    for c_sta in stations_all:\n",
    "        # Test if this varible was measured at this station\n",
    "        if ((c_sta,c_var) in c_dict):\n",
    "            all_vars[c_sta] = c_dict[(c_sta,c_var)]\n",
    "        else: # Variable doesn't exists at this station so pad it with -9999 (needed to merge into one netcdf file)\n",
    "            index_csta = time_index[c_sta]\n",
    "            df_missing = pd.DataFrame(index=index_csta, columns=[c_var])\n",
    "            #df_missing = df_missing.fillna(-9999)\n",
    "            all_vars[c_sta] = df_missing\n",
    "\n",
    "    # Concatenate each variable by stations\n",
    "    c_obs_all = pd.concat(all_vars,axis=0,keys=stations_all)\n",
    "    #c_obs_all = pd.DataFrame(c_obs_all) # not needed\n",
    "    \n",
    "    # Convert to xray and add to list\n",
    "    ds = xr.Dataset.from_dataframe(c_obs_all)\n",
    "    # Add to list and rename variables\n",
    "    ds_list.append(ds.rename({'level_0':'station','TIMESTAMP':'time'}))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<xarray.Dataset>\n",
       "Dimensions:                               (station: 12, time: 131425)\n",
       "Coordinates:\n",
       "  * station                               (station) object 'BNS' 'BRP' 'BWH' ...\n",
       "  * time                                  (time) datetime64[ns] 2012-10-01 ...\n",
       "Data variables:\n",
       "    Soil Moisture C                       (station, time) float64 nan nan ...\n",
       "    Downward Solar Radiation              (station, time) float64 nan nan ...\n",
       "    Soil Moisture E                       (station, time) float64 nan nan ...\n",
       "    Total Pressure Adjusted to Sea-level  (station, time) float64 nan nan ...\n",
       "    Air Moisture Content A                (station, time) float64 nan nan ...\n",
       "    Wind Direction at A                   (station, time) float64 nan nan ...\n",
       "    Scalar Wind Speed B                   (station, time) float64 nan nan ...\n",
       "    Scalar Wind Speed A                   (station, time) float64 nan nan ...\n",
       "    Soil Moisture A                       (station, time) float64 nan nan ...\n",
       "    Upward Terrestrial Rad                (station, time) float64 nan nan ...\n",
       "    Soil Temperature E                    (station, time) float64 nan nan ...\n",
       "    Snow Depth A                          (station, time) float64 nan nan ...\n",
       "    Snow Water Equivelent A               (station, time) float64 nan nan ...\n",
       "    Soil Temperature B                    (station, time) float64 nan nan ...\n",
       "    Soil Moisture D                       (station, time) float64 nan nan ...\n",
       "    Downward Terrestrial Rad              (station, time) float64 nan nan ...\n",
       "    Incremental Precipitation B           (station, time) float64 nan nan ...\n",
       "    Soil Temperature A                    (station, time) float64 nan nan ...\n",
       "    Snow Layer Temperature A              (station, time) object nan nan nan ...\n",
       "    Soil Temperature D                    (station, time) float64 nan nan ...\n",
       "    Soil Moisture B                       (station, time) float64 nan nan ...\n",
       "    Soil Temperature C                    (station, time) float64 nan nan ...\n",
       "    Total Pressure Unadjusted A           (station, time) float64 nan nan ...\n",
       "    Air temperature A                     (station, time) float64 nan nan ...\n",
       "    Soil Heat Flux  A                     (station, time) float64 nan nan ...\n",
       "    Incremental Precipitation A           (station, time) float64 nan nan ...\n",
       "    Snow Depth QC value                   (station, time) object nan nan nan ...\n",
       "    Upward Solar Radiation                (station, time) float64 nan nan ..."
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Combine all variable Datasets using xray.update()\n",
    "ds_all = xr.Dataset()\n",
    "[ds_all.update(c_ds) for c_ds in ds_list]\n",
    "ds_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Add variable attributes (units), and fix variable names (remove spaces)\n",
    "for cvar in ds_all.data_vars:\n",
    "    # add units as attributes\n",
    "    ds_all.get(cvar).attrs['unit']   = units_all[cvar]\n",
    "    # Remove spaces in variable names\n",
    "    ds_all.rename({cvar:cvar.replace(\" \",\"\")},inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Tell xray TIMESTAMP is a datetime (it forgets for some reason)  and set time zone\n",
    "#ds_all['time'] = pd.to_datetime(ds_all.time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>lat</th>\n",
       "      <th>long</th>\n",
       "      <th>elevation(m)</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>station</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>BNS</th>\n",
       "      <td>50.820920</td>\n",
       "      <td>-115.213910</td>\n",
       "      <td>2099</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PWL</th>\n",
       "      <td>50.825980</td>\n",
       "      <td>-115.198200</td>\n",
       "      <td>2100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>FRS</th>\n",
       "      <td>50.838230</td>\n",
       "      <td>-115.215780</td>\n",
       "      <td>2330</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CRN</th>\n",
       "      <td>50.670700</td>\n",
       "      <td>-123.191700</td>\n",
       "      <td>2205</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>BRP</th>\n",
       "      <td>50.760610</td>\n",
       "      <td>-115.367150</td>\n",
       "      <td>2260</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>FLG</th>\n",
       "      <td>50.830040</td>\n",
       "      <td>-115.228510</td>\n",
       "      <td>2565</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PYT</th>\n",
       "      <td>51.509340</td>\n",
       "      <td>-123.442020</td>\n",
       "      <td>2237</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>BWH</th>\n",
       "      <td>51.635180</td>\n",
       "      <td>-116.490280</td>\n",
       "      <td>2421</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CRG</th>\n",
       "      <td>50.670590</td>\n",
       "      <td>-123.191640</td>\n",
       "      <td>2211</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>FRG</th>\n",
       "      <td>50.836420</td>\n",
       "      <td>-115.220490</td>\n",
       "      <td>2327</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SIB</th>\n",
       "      <td>51.055630</td>\n",
       "      <td>-114.869420</td>\n",
       "      <td>1490</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>HLN</th>\n",
       "      <td>51.693027</td>\n",
       "      <td>-116.420957</td>\n",
       "      <td>2535</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               lat        long   elevation(m)\n",
       "station                                      \n",
       "BNS      50.820920 -115.213910           2099\n",
       "PWL      50.825980 -115.198200           2100\n",
       "FRS      50.838230 -115.215780           2330\n",
       "CRN      50.670700 -123.191700           2205\n",
       "BRP      50.760610 -115.367150           2260\n",
       "FLG      50.830040 -115.228510           2565\n",
       "PYT      51.509340 -123.442020           2237\n",
       "BWH      51.635180 -116.490280           2421\n",
       "CRG      50.670590 -123.191640           2211\n",
       "FRG      50.836420 -115.220490           2327\n",
       "SIB      51.055630 -114.869420           1490\n",
       "HLN      51.693027 -116.420957           2535"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Add meta data for each station\n",
    "ds_all = ds_all.merge({'Elevation': ('station',metadata[' elevation(m)'])})\n",
    "ds_all = ds_all.merge({'Lat': ('station',metadata[' lat'])})\n",
    "ds_all = ds_all.merge({'Lon': ('station',metadata[' long'])})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<xarray.Dataset>\n",
       "Dimensions:                           (station: 12, time: 131425)\n",
       "Coordinates:\n",
       "  * station                           (station) object 'BNS' 'BRP' 'BWH' ...\n",
       "  * time                              (time) datetime64[ns] 2012-10-01 ...\n",
       "    Elevation                         (station) int64 2099 2100 2330 2205 ...\n",
       "    Lat                               (station) float64 50.82 50.83 50.84 ...\n",
       "    Lon                               (station) float64 -115.2 -115.2 -115.2 ...\n",
       "Data variables:\n",
       "    SoilMoistureC                     (station, time) float64 nan nan nan ...\n",
       "    DownwardSolarRadiation            (station, time) float64 nan nan nan ...\n",
       "    SoilMoistureE                     (station, time) float64 nan nan nan ...\n",
       "    TotalPressureAdjustedtoSea-level  (station, time) float64 nan nan nan ...\n",
       "    AirMoistureContentA               (station, time) float64 nan nan nan ...\n",
       "    WindDirectionatA                  (station, time) float64 nan nan nan ...\n",
       "    ScalarWindSpeedB                  (station, time) float64 nan nan nan ...\n",
       "    ScalarWindSpeedA                  (station, time) float64 nan nan nan ...\n",
       "    SoilMoistureA                     (station, time) float64 nan nan nan ...\n",
       "    UpwardTerrestrialRad              (station, time) float64 nan nan nan ...\n",
       "    SoilTemperatureE                  (station, time) float64 nan nan nan ...\n",
       "    SnowDepthA                        (station, time) float64 nan nan nan ...\n",
       "    SnowWaterEquivelentA              (station, time) float64 nan nan nan ...\n",
       "    SoilTemperatureB                  (station, time) float64 nan nan nan ...\n",
       "    SoilMoistureD                     (station, time) float64 nan nan nan ...\n",
       "    DownwardTerrestrialRad            (station, time) float64 nan nan nan ...\n",
       "    IncrementalPrecipitationB         (station, time) float64 nan nan nan ...\n",
       "    SoilTemperatureA                  (station, time) float64 nan nan nan ...\n",
       "    SnowLayerTemperatureA             (station, time) object nan nan nan nan ...\n",
       "    SoilTemperatureD                  (station, time) float64 nan nan nan ...\n",
       "    SoilMoistureB                     (station, time) float64 nan nan nan ...\n",
       "    SoilTemperatureC                  (station, time) float64 nan nan nan ...\n",
       "    TotalPressureUnadjustedA          (station, time) float64 nan nan nan ...\n",
       "    AirtemperatureA                   (station, time) float64 nan nan nan ...\n",
       "    SoilHeatFluxA                     (station, time) float64 nan nan nan ...\n",
       "    IncrementalPrecipitationA         (station, time) float64 nan nan nan ...\n",
       "    SnowDepthQCvalue                  (station, time) object nan nan nan nan ...\n",
       "    UpwardSolarRadiation              (station, time) float64 nan nan nan ..."
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Make meta data coordiates from variables\n",
    "ds_all.set_coords(['Elevation','Lat','Lon'], inplace=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<xarray.DataArray 'time' (time: 131425)>\n",
       "array(['2012-10-01T00:00:00.000000000', '2012-10-01T00:15:00.000000000',\n",
       "       '2012-10-01T00:30:00.000000000', ...,\n",
       "       '2016-06-30T23:30:00.000000000', '2016-06-30T23:45:00.000000000',\n",
       "       '2016-07-01T00:00:00.000000000'], dtype='datetime64[ns]')\n",
       "Coordinates:\n",
       "  * time     (time) datetime64[ns] 2012-10-01 2012-10-01T00:15:00 ..."
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds_all.time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Export to netcdf\n",
    "ds_all.to_netcdf(cfileout,format='netcdf4') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Trim to nov 2014 storm\n",
    "ds_storm = ds_all.sel(time=pd.date_range(start=datetime(2014,10,1),end=datetime(2014,12,15),freq='15min'))\n",
    "ds_storm.to_netcdf(cfileout_storm,format='netcdf4') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
